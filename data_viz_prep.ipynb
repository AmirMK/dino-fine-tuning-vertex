{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794a75a4-0a77-4aa8-acaf-b2f83bd95952",
   "metadata": {},
   "source": [
    "# Embedding Clustering & Visualization Pipeline\n",
    "\n",
    "This notebook processes an input file of vector embeddings, applies dimensionality reduction and clustering, and produces a 2D projection suitable for interactive visualization (e.g., in Streamlit). It also supports hyperparameter tuning of the clustering step to help identify optimal parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- **Input file format**:  \n",
    "  A Parquet file (`.parquet`) with at least:  \n",
    "  - `file` – string identifier for each row (e.g., filename)  \n",
    "  - `embedding` – list of floats of equal length (e.g., 750D vectors)  \n",
    "\n",
    "- **Dependencies**:  \n",
    "  - `numpy`, `pandas`, `tqdm`  \n",
    "  - `scikit-learn` (`PCA`, `normalize`, `silhouette_score`)  \n",
    "  - `hdbscan`  \n",
    "  - `umap-learn`  \n",
    "\n",
    "---\n",
    "\n",
    "## Processing Steps\n",
    "\n",
    "1. **Load embeddings**  \n",
    "   - Reads parquet file into a DataFrame.  \n",
    "   - Ensures embeddings are equal-length vectors.  \n",
    "\n",
    "2. **PCA reduction (denoising)**  \n",
    "   - Optionally L2-normalizes embeddings.  \n",
    "   - Reduces dimensionality (e.g., 750 → 300).  \n",
    "   - Adds a new column: `embedding_pca`.  \n",
    "\n",
    "3. **Clustering with HDBSCAN**  \n",
    "   - L2-normalizes vectors (so Euclidean ≈ Cosine).  \n",
    "   - Runs HDBSCAN to assign clusters.  \n",
    "   - Outputs:  \n",
    "     - `cluster` – cluster ID per point (-1 = noise)  \n",
    "     - `probability` – confidence score  \n",
    "\n",
    "4. **Hyperparameter tuning (optional)**  \n",
    "   - Runs clustering across a parameter grid:  \n",
    "     - `min_cluster_size`  \n",
    "     - `min_samples`  \n",
    "     - `cluster_selection_epsilon`  \n",
    "   - Collects metrics for each run:  \n",
    "     - Silhouette score (with/without noise)  \n",
    "     - Number of clusters  \n",
    "     - % of points clustered  \n",
    "   - Results are stored in `df_grid`, sorted by silhouette score.  \n",
    "\n",
    "5. **UMAP reduction (visualization)**  \n",
    "   - Reduces embeddings to 2D (`dim1`, `dim2`).  \n",
    "   - Uses normalized vectors when available.  \n",
    "\n",
    "6. **Save results**  \n",
    "   - Final DataFrame includes:  \n",
    "     - PCA embeddings (`embedding_pca`)  \n",
    "     - Clusters & probabilities  \n",
    "     - UMAP coordinates (`dim1`, `dim2`)  \n",
    "   - Written to parquet: `embeddings_result.parquet`  \n",
    "\n",
    "This output can be directly consumed by a Streamlit app for interactive visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10a640a-a995-45ea-8fbf-12bd1f459db7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import hdbscan\n",
    "import umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bdfcd14-c0c0-4bf7-a8b9-4c1754e8cf9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_embeddings(parquet_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load embeddings parquet file into a DataFrame.\n",
    "    Expects columns: 'file' (str), 'embedding' (list of floats).\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    print(f\"✅ Loaded {len(df):,} rows from {parquet_path}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    return df\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _column_of_lists_to_matrix(df: pd.DataFrame, col: str) -> np.ndarray:\n",
    "    \"\"\"Safely stack a df column of equal-length lists/arrays into a 2D float32 matrix.\"\"\"\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in DataFrame.\")\n",
    "    # Convert to ndarray\n",
    "    arrs = df[col].to_numpy()\n",
    "    # Ensure each entry is array-like and same length\n",
    "    first_len = len(arrs[0])\n",
    "    if not all(hasattr(x, \"__len__\") and len(x) == first_len for x in arrs):\n",
    "        raise ValueError(f\"Column '{col}' must contain equal-length vectors.\")\n",
    "    X = np.vstack(arrs).astype(np.float32, copy=False)\n",
    "    return X\n",
    "\n",
    "def _matrix_to_list_column(X: np.ndarray) -> list:\n",
    "    \"\"\"Convert 2D matrix (N, D) to list-of-lists for storing back in a df column.\"\"\"\n",
    "    return [row.astype(float).tolist() for row in X]\n",
    "\n",
    "\n",
    "# ---------- 1) PCA: 750D -> 300D ----------\n",
    "def pca_reduce(\n",
    "    df: pd.DataFrame,\n",
    "    source_col: str = \"embedding\",\n",
    "    target_col: str = \"embedding_pca\",\n",
    "    n_components: int = 300,\n",
    "    l2_normalize: bool = True,\n",
    "    random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply optional L2-normalization (row-wise) then PCA to 'n_components'.\n",
    "    Adds/updates df[target_col] with list-of-floats length n_components.\n",
    "    \"\"\"\n",
    "    X = _column_of_lists_to_matrix(df, source_col)\n",
    "    if l2_normalize:\n",
    "        X = normalize(X, norm=\"l2\")\n",
    "\n",
    "    pca = PCA(n_components=n_components, random_state=random_state)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    df[target_col] = _matrix_to_list_column(X_pca)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------- 2) Clustering: HDBSCAN ----------\n",
    "\n",
    "def cluster_hdbscan_silhouette(\n",
    "    df: pd.DataFrame,\n",
    "    source_col: str = \"embedding_pca\",      # or \"embedding\"\n",
    "    min_cluster_size: int = 10,             # 30\n",
    "    metric: str = \"cosine\",                 # kept for API compatibility; ignored (we use euclidean after L2 norm)\n",
    "    l2_normalize: bool = True,\n",
    "    cluster_selection_epsilon: float = 0.0,\n",
    "    min_samples: int = 5,\n",
    "    normalized_col: str = \"embedding_norm\"\n",
    "    \n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Run HDBSCAN clustering and return silhouette score.\n",
    "    - L2-normalizes vectors (so Euclidean ≡ Cosine).\n",
    "    - Returns silhouette score (higher = better separation).\n",
    "      If clustering produces only 1 cluster or only noise, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    # stack to (N, D)\n",
    "    X = _column_of_lists_to_matrix(df, source_col)\n",
    "\n",
    "    # L2-normalize (row-wise) to make euclidean ≡ cosine\n",
    "    if l2_normalize:\n",
    "        Xn = normalize(X, norm=\"l2\")\n",
    "    else:\n",
    "        Xn = X\n",
    "\n",
    "    # Save normalized vectors back to df\n",
    "    df[normalized_col] = [row.astype(float).tolist() for row in Xn]\n",
    "\n",
    "    # Run HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric=\"euclidean\",  # force euclidean after L2 norm\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon\n",
    "    )\n",
    "    labels = clusterer.fit_predict(Xn)\n",
    "\n",
    "    # Count clusters and % clustered\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    mask = labels != -1\n",
    "    pct_clustered = float(mask.mean() * 100.0)\n",
    "    \n",
    "    # silhouette requires at least 2 clusters (ignores noise as -1)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    if n_clusters <= 1:\n",
    "        return None  # silhouette score undefined in this case\n",
    "    \n",
    "    mask = labels != -1\n",
    "    if mask.sum() == 0:\n",
    "        return None  # no clustered points\n",
    "    unique_clusters = set(labels[mask])\n",
    "    if len(unique_clusters) <= 1:\n",
    "        return None  # silhouette requires at least 2 clusters\n",
    "    \n",
    "    score_all = silhouette_score(Xn, labels, metric=\"euclidean\")\n",
    "    score = silhouette_score(Xn[mask], labels[mask], metric=\"euclidean\")\n",
    "\n",
    "    return score_all,score,n_clusters, pct_clustered\n",
    "\n",
    "\n",
    "def cluster_hdbscan(\n",
    "    df: pd.DataFrame,\n",
    "    source_col: str = \"embedding_pca\",      # or \"embedding\"\n",
    "    target_col: str = \"cluster\",\n",
    "    min_cluster_size: int = 10, #30\n",
    "    metric: str = \"cosine\",                  # kept for API compatibility; ignored (we use euclidean after L2 norm)\n",
    "    l2_normalize: bool = True,\n",
    "    cluster_selection_epsilon: float = 0.0,\n",
    "    min_samples: int =5,\n",
    "    random_state: int = 42,\n",
    "    normalized_col: str = \"embedding_norm\",   # NEW: where to store normalized vectors\n",
    "    prob_col: str = \"probability\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cosine-equivalent HDBSCAN: L2-normalize vectors, then cluster with Euclidean.\n",
    "    - Adds/updates df[normalized_col] with the normalized vectors (list per row).\n",
    "    - Adds/updates df[target_col] with integer labels (-1 = noise).\n",
    "    \"\"\"\n",
    "    # stack to (N, D)\n",
    "    X = _column_of_lists_to_matrix(df, source_col)\n",
    "\n",
    "    # L2-normalize (row-wise) to make euclidean ≡ cosine\n",
    "    if l2_normalize:\n",
    "        Xn = normalize(X, norm=\"l2\")\n",
    "    else:\n",
    "        Xn = X  # not recommended for cosine equivalence\n",
    "\n",
    "    # Save normalized vectors back to df (as lists) so you can reuse without recomputing\n",
    "    df[normalized_col] = [row.astype(float).tolist() for row in Xn]\n",
    "\n",
    "    # HDBSCAN with Euclidean on normalized vectors (cosine-equivalent)\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric=\"euclidean\",                   # force euclidean after L2 norm\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon\n",
    "    )\n",
    "    labels = clusterer.fit_predict(Xn)\n",
    "    df[target_col] = labels.astype(int)\n",
    "    df[prob_col] = clusterer.probabilities_.astype(float)\n",
    "    return df\n",
    "\n",
    "# ---------- 3) 2D UMAP for visualization ----------\n",
    "def umap_reduce(\n",
    "    df: pd.DataFrame,\n",
    "    source_col: str = \"embedding_pca\",   # will auto-switch to normalized_col if present\n",
    "    dim1_col: str = \"dim1\",\n",
    "    dim2_col: str = \"dim2\",\n",
    "    n_neighbors: int = 30,\n",
    "    min_dist: float = 0.1,\n",
    "    metric: str = \"cosine\",              # kept for API compat; ignored if using normalized_col\n",
    "    l2_normalize: bool = True,           # ignored if using normalized_col\n",
    "    random_state: int = 42,\n",
    "    normalized_col: str = \"embedding_norm\",\n",
    "    prefer_normalized: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run UMAP to 2D and add/overwrite 'dim1','dim2'.\n",
    "    If `prefer_normalized` and `normalized_col` exists, use it and force metric='euclidean'\n",
    "    (cosine-equivalent after L2 normalization). Otherwise, behaves like the original.\n",
    "    \"\"\"\n",
    "    use_norm = prefer_normalized and (normalized_col in df.columns)\n",
    "\n",
    "    # Pick source\n",
    "    col = normalized_col if use_norm else source_col\n",
    "    X = _column_of_lists_to_matrix(df, col)\n",
    "\n",
    "    # If not using pre-normalized vectors, optionally normalize for cosine/euclidean geometry\n",
    "    if not use_norm and l2_normalize and metric in (\"cosine\", \"euclidean\"):\n",
    "        from sklearn.preprocessing import normalize\n",
    "        X = normalize(X, norm=\"l2\")\n",
    "\n",
    "    # Metric choice:\n",
    "    # - If using normalized vectors, euclidean == cosine (up to scaling), so force 'euclidean'\n",
    "    #   to avoid any environment metric quirks.\n",
    "    umap_metric = \"euclidean\" if use_norm else metric\n",
    "\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric=umap_metric,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    X_2d = reducer.fit_transform(X)\n",
    "    df[dim1_col] = X_2d[:, 0].astype(float)\n",
    "    df[dim2_col] = X_2d[:, 1].astype(float)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54671acf-48a0-4f10-ab6f-5f8035a7f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfile = 'dino_embedding' \n",
    "df = load_embeddings(f'{dino_embedding}.parquet') #Original\n",
    "df = pca_reduce(df, n_components=300)         # adds 'embedding_pca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ddbf9-882a-4bd8-ad78-2dbfcb87aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: run clustring over set of hyper parameters \n",
    "# --- define your grids (tweak as you like) ---\n",
    "grid_min_cluster_size = [10, 15, 20, 30, 40, 60]\n",
    "grid_min_samples      = [1, 2, 5, 10, 15, 20, None]  # None => most conservative\n",
    "grid_epsilon          = [0.0, 0.02, 0.05, 0.1, 0.15, 0.2, 0.3]\n",
    "\n",
    "results = []\n",
    "\n",
    "combos = list(itertools.product(grid_min_cluster_size, grid_min_samples, grid_epsilon))\n",
    "\n",
    "\n",
    "for mcs, ms, eps in tqdm(combos, desc=\"Grid search progress\"):\n",
    "    try:\n",
    "        s_all,s,n_clusters, pct_clustered = cluster_hdbscan_silhouette(\n",
    "            df,\n",
    "            source_col=\"embedding_pca\",\n",
    "            min_cluster_size=mcs,\n",
    "            min_samples=ms,\n",
    "            cluster_selection_epsilon=eps,\n",
    "\n",
    "        )\n",
    "        # If silhouette is undefined (None), store NaN so we can sort/filter easily\n",
    "        s_val = np.nan if s is None else float(s)\n",
    "        s_all_val = np.nan if s_all is None else float(s_all)\n",
    "    except Exception as e:\n",
    "        # In case any combo fails, record as NaN (or log e)\n",
    "        s_val = np.nan\n",
    "        s_all_val = np.nan\n",
    "                \n",
    "\n",
    "    results.append({\n",
    "        \"min_cluster_size\": mcs,\n",
    "        \"min_samples\": ms if ms is not None else \"None\",\n",
    "        \"cluster_selection_epsilon\": eps,\n",
    "        \"silhouette_include_noise\": s_all_val,\n",
    "        \"silhouette_excl_noise\": s_val,\n",
    "        \"number_clusters\": n_clusters,\n",
    "        \"pct_clustered\": pct_clustered\n",
    "    })\n",
    "\n",
    "df_grid = pd.DataFrame(results).sort_values(\n",
    "    by=\"silhouette_excl_noise\", ascending=False, na_position=\"last\"\n",
    ").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab666b8-6305-409e-a7b1-56254fe7e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cluster_hdbscan(df,\n",
    "            source_col=\"embedding_pca\",\n",
    "            min_cluster_size=15,\n",
    "            min_samples=1,\n",
    "            cluster_selection_epsilon=.02\n",
    "                    )\n",
    "    \n",
    "    #df, source_col=\"embedding_pca\")      # adds 'cluster'\n",
    "df = umap_reduce(df, source_col=\"embedding_pca\")          # adds 'dim1','dim2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428b8ed-17b6-44b3-bed1-7da2c56cb61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfile = 'embeddings_result'\n",
    "df.to_parquet(f\"{outputfile}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
